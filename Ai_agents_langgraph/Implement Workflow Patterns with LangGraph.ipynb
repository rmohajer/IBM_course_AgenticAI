{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004de1b9-aa31-41de-826f-fd58500fd1a8",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e2d58-dc35-4987-bedd-42de57e44b22",
   "metadata": {},
   "source": [
    "#  Implement Workflow Patterns with LangGraph\n",
    "\n",
    "Estimated time needed: **45** minutes\n",
    "\n",
    "## Introduction\n",
    "In this lab, you'll master the three essential workflow patterns that transform individual AI models into sophisticated, coordinated systems. Through hands-on projects using LangGraph and LangChain, you'll build job application assistants, intelligent task routers, and multilingual processors that demonstrate real-world multi-agent coordination.\n",
    "These proven patterns - Sequential Agent Coordination, Intent-Based Routing, and Parallel Agent Execution - form the foundation of every enterprise AI system. By the end, you'll have the architectural knowledge to create AI applications that intelligently orchestrate multiple specialized agents to solve complex problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e508b2e-2a60-42ca-ab38-bc5ba75c7d0f",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Prompt-Chaining\">Prompt Chaining</a></li>\n",
    "    <li><a href=\"#Resume-Summary-Agent\">Resume Summary Agent</a></li>\n",
    "    <li><a href=\"#Generate-Cover-Letter-Agent\">Generate Cover Letter Agent</a></li>\n",
    "    <li><a href=\"#LangGraph-Workflow\">LangGraph Workflow</a></li>\n",
    "    <li><a href=\"#Initializing-the-LangGraph-Workflow\">Initializing the LangGraph Workflow</a></li>\n",
    "    <li><a href=\"#Workflow-Pattern:-Routing\">Workflow Pattern: Routing</a></li>\n",
    "    <li><a href=\"#Workflow-Pattern:-Parallelization\">Workflow Pattern: Parallelization</a></li>\n",
    "    </ol>\n",
    "\n",
    "<a href=\"#Exercises:-Building-a-Multi-Agent-Routing-System\">Exercises: Building a Multi-Agent Routing System</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce203b4-6170-426f-b70f-461abf05021a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041c1bb-c4bc-480b-befb-b8b7782ee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install langchain-openai==0.3.27\n",
    "# %pip install langgraph==0.6.6\n",
    "# %pip install pygraphviz==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezamohajerpoor/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_mistralai/embeddings.py:186: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model_id = \"openai/gpt-oss-120b\" # \tqwen/qwen3-32b  openai/gpt-oss-120b llama3-8b-8192\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "finnhub_api_key = os.getenv(\"FINNHUB_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model=model_id,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            verbose=1)\n",
    "\n",
    "embeddings = MistralAIEmbeddings(\n",
    "        model=\"mistral-embed\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52f49234-d097-4416-8489-c4862696333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END,START\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79be4492-84e6-4b48-b170-05b33ff5c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_workflow_info(workflow, app=None):\n",
    "    \"\"\"Prints comprehensive information about a LangGraph workflow.\"\"\"\n",
    "    print(\"WORKFLOW INFORMATION\")\n",
    "    print(\"====================\")\n",
    "    print(f\"Nodes: {workflow.nodes}\")\n",
    "    print(f\"Edges: {workflow.edges}\")\n",
    "\n",
    "    \n",
    "    # Use getter method for finish points if available\n",
    "    try:\n",
    "        finish_points = workflow.finish_points\n",
    "        print(f\"Finish points: {finish_points}\")\n",
    "    except:\n",
    "        try:\n",
    "            # Alternative approaches\n",
    "            print(f\"Finish point: {workflow._finish_point}\")\n",
    "        except:\n",
    "            print(\"Finish points attribute not directly accessible\")\n",
    "    \n",
    "    if app:\n",
    "        print(\"\\nWorkflow Visualization:\")\n",
    "        from IPython.display import display\n",
    "        display(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c7912-8d6a-4781-8345-dd475eb75154",
   "metadata": {},
   "source": [
    "\n",
    "Now we are going to instantiate the `ChatOpenAI` class with the `gpt-4o-mini` model. This instance, stored in the variable `llm`, will be used to handle all LLM-based interactions throughout our workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d5f09-43fc-4250-92ef-1e4450f9b4a0",
   "metadata": {},
   "source": [
    "### Prompt Chaining\n",
    "\n",
    "Prompt Chaining is a workflow design pattern where complex tasks are decomposed into a sequence of LLM (Large Language Model) calls. Each step depends on the output of the previous one, allowing for step-by-step refinement or evolution of the data being processed. This method mirrors how humans tackle multifaceted problems—by breaking them down into manageable steps.\n",
    "\n",
    "It leverages **function calling**, **sequential chaining** and or **AI Agents** , often implemented using frameworks such as **LangChain**, **LangGraph**, or even custom scripts. The key is modularity and clarity—each node (or step) has a specific role in the overall pipeline. Each link in the chain is a tool call or Agent the structure of which is something like the following:\n",
    "\n",
    "#### Typical Structure:\n",
    "- **Step 1:** Initial LLM prompt (for example, generate a draft)\n",
    "- **Step 2:** Refinement prompt (for example, improve style, tone)\n",
    "- **Step 3:** Evaluation or formatting (for example, convert to specific format or assess quality)\n",
    "\n",
    "This pattern also allows injecting **external tools** between steps (for example, validation, summarization, keyword extraction).\n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cases:\n",
    "- Generating blog posts or marketing copy step-by-step (idea → outline → paragraph → polish)\n",
    "- Automated report generation (for example, extract → analyze → summarize)\n",
    "- Educational content creation (for example, topic → questions → answers → explanations)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36711bec-d4ab-4384-b312-8e1740b176ce",
   "metadata": {},
   "source": [
    "### Use Case: Prompt Chaining — Job Application Assistant\n",
    "\n",
    "In this workflow, we are going to build a simple **job application assistant** using the Prompt Chaining pattern. The goal is to help a user create a **personalized cover letter** from a given job description.\n",
    "\n",
    "We will break the task into two sequential steps:\n",
    "1. First, the LLM will read the **job description** and generate a **resume summary** tailored to that role.\n",
    "2. Then, using this summary, the LLM will generate a professional **cover letter** suitable for submitting with a job application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3415c-bf6b-40ee-8fa5-12606f475cf5",
   "metadata": {},
   "source": [
    "For this workflow, we need a structured way to manage the data passed between each step. To achieve this, we define a `ChainState` using `TypedDict`. This state will act as a shared container for all intermediate and final outputs.\n",
    "\n",
    "- First, we have `job_description`, which will store the input provided by the user, typically a job posting or role summary.\n",
    "- Then, we include `resume_summary`, which will hold the tailored summary of the applicant’s profile generated by the LLM based on the job description.\n",
    "- Finally, we have `cover_letter`, where the personalized cover letter will be stored after the second LLM call completes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff472eac-5d54-4a44-9eeb-c8a890b6b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainState(TypedDict):\n",
    "    job_description: str\n",
    "    resume_summary: str\n",
    "    cover_letter: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d765ea-5752-4fe3-831a-d99eed9aaf03",
   "metadata": {},
   "source": [
    "We can represent an instance of the ChainState class as a Python dictionary. In the Prompt Chaining pattern, here's how the state evolves:\n",
    "```python\n",
    "\n",
    "state = {\n",
    "    \"job_description\": \"\", \n",
    "    \"resume_summary\": \"\",\n",
    "    \"cover_letter\": \"\"\n",
    "}\n",
    "```\n",
    "```Initial State```: The workflow begins with a dictionary containing only the job description:\n",
    "\n",
    "```python\n",
    "state = {\n",
    "    \"job_description\": \"\"We are looking for a data scientist with experience in machine learning, NLP..\", \n",
    "    \"resume_summary\": \"\",\n",
    "    \"cover_letter\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "The  next state is given by:\n",
    "```python\n",
    "state = {\n",
    "    \"job_description\": \"\"We are looking for a data scientist with experience in machine learning, NLP..\", \n",
    "    \"resume_summary\": \"Results-driven data scientist with expertise in machine learning...\",\n",
    "    \"cover_letter\": \"\"\n",
    "}\n",
    "```\n",
    "and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8fdbf-313d-4030-a4dc-b53b30294da5",
   "metadata": {},
   "source": [
    "Now, let's see how to build a prompt chain using a series of LLM calls—as in, agents. We'll demonstrate the process, which is summarized in the following image. The input is a state variable containing the job description. This is first passed to the Resume Summary agent, which generates a summary of the resume and stores it under the key resume_summary. That output is then used as input for the Generate Cover Letter agent, which produces a cover letter and fills in the key cover_letter.\n",
    "![Screenshot 2025-04-24 at 12.03.31 PM.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hseuyL5ddwU6cvKXrVGQnw/Screenshot%202025-04-24%20at%2012-03-31%E2%80%AFPM.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803afe63-3804-46da-a9d4-ac5bf1da3a62",
   "metadata": {},
   "source": [
    "### Resume Summary Agent\n",
    "\n",
    "In LLM workflows, an \"agent\" is created through a prompt that gives the LLM specific instructions and persona. The generate_resume_summary node demonstrates this by transforming the LLM into a \"resume assistant\" through its prompt. This node receives the state containing the job description, processes it using the agent created by the prompt, and returns an updated state with the new resume summary.\n",
    "Nodes provide the workflow structure while prompts define agent capabilities. The state object serves as shared memory between nodes, allowing each agent to build upon previous work while using the same underlying LLM instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08e05cc-ad72-4895-9627-419609290340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_resume_summary(state: ChainState) -> ChainState:\n",
    "    prompt = f\"\"\"\n",
    "You're a resume assistant. Read the following job description and summarize the key qualifications and experience the ideal candidate should have, phrased as if from the perspective of a strong applicant's resume summary.\n",
    "\n",
    "Job Description:\n",
    "{state['job_description']}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {**state, \"resume_summary\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98248e-8a09-471c-84e2-0c1bd0b70ff0",
   "metadata": {},
   "source": [
    "The agent starts with this initial state and passes it to the first node, generate_resume_summary. Inside that function, it can access the job description using ```state['job_description']```, which will serve as the input later on.\n",
    "\n",
    "When the function returns, it uses Python’s dictionary unpacking syntax: **state**. This means “create a new dictionary that includes all the key-value pairs from the original state dictionary, and then add or update specific keys.” In this case, it updates the \"resume_summary\" field with the newly generated content.\n",
    "The state varable would look like this:\n",
    "\n",
    "```python\n",
    "state = {\n",
    "    \"job_description\": \"\"We are looking for a data scientist with experience in machine learning, NLP..\", \n",
    "    \"resume_summary\": \"Results-driven data scientist with expertise in machine learning...\",\n",
    "    \"cover_letter\": \"\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb1c90-9582-4d20-b381-941cd6963386",
   "metadata": {},
   "source": [
    "### Generate Cover Letter Agent\n",
    "\n",
    "\n",
    "The ```generate_cover_letter``` node defines our second agent in the workflow. This function creates a specialized agent through its prompt This agent accesses both ```state['resume_summary']``` and ```state['job_description'] ```from the current state, leveraging both the output from the previous agent and the original input. The prompt transforms the LLM into a cover letter specialist that synthesizes these elements into a tailored application document. The agent's output is then added to the state dictionary under the ```cover_letter``` key, completing the workflow chain with a state object containing all three key elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af64050-bc96-4bcb-8371-7215863c2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(state: ChainState) -> ChainState:\n",
    "    prompt = f\"\"\"\n",
    "You're a cover letter writing assistant. Using the resume summary below, write a professional and personalized cover letter for the following job.\n",
    "\n",
    "Resume Summary:\n",
    "{state['resume_summary']}\n",
    "\n",
    "Job Description:\n",
    "{state['job_description']}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {**state, \"cover_letter\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2be91c-60d9-4afb-9c3b-64b8713913de",
   "metadata": {},
   "source": [
    "## LangGraph Workflow \n",
    "### Initializing the LangGraph Workflow\n",
    "\n",
    "\n",
    "\n",
    "This line creates a new `StateGraph` instance and configures it with our `ChainState` definition. This crucial step establishes the workflow's foundation by specifying the structure of data that will flow through the nodes. The `StateGraph` uses the `ChainState` TypedDict to validate data types for each field (`job_description`, `resume_summary`, and `cover_letter`), ensuring proper information passing between nodes. This data contract enables LangGraph to manage state transitions efficiently as the workflow progresses from node to node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed08fda-5186-42b1-91a1-083422d414fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12149fc50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ChainState)\n",
    "workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1adff-fec5-4dc0-b988-cba22bd686d8",
   "metadata": {},
   "source": [
    "We are adding two nodes to our workflow graph. Each node represents a distinct step in the prompt chaining process:\n",
    "\n",
    "- `\"generate_resume_summary\"` is the first node, which generates a tailored summary based on the job description.\n",
    "- `\"generate_cover_letter\"` is the second node, which uses that summary to create a personalized cover letter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "887d3434-9c56-4792-8771-a87e5418a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12149fc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"generate_resume_summary\", generate_resume_summary)\n",
    "workflow.add_node(\"generate_cover_letter\", generate_cover_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94083ada-98cb-4065-bf07-f7f499dd1078",
   "metadata": {},
   "source": [
    "Setting the **entry point** of the workflow to `\"generate_resume_summary\"`. This means that when the workflow is executed, it will start with this node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186f6547-5134-44c4-89fa-69f568c56f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12149fc50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"generate_resume_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe328df5-4805-452b-8cb4-c2555ca78c51",
   "metadata": {},
   "source": [
    "Now we are defining the **connection between two nodes** by adding an edge from `\"generate_resume_summary\"` to `\"generate_cover_letter\"`. This tells the workflow to pass the state from the first node to the second, forming a sequential chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3080207-15b6-4c58-ac1e-e3644488dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12149fc50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"generate_resume_summary\", \"generate_cover_letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02f063-56eb-416b-94e1-4272a5358444",
   "metadata": {},
   "source": [
    "Now we are marking `\"generate_cover_letter\"` as the final node, indicating where the workflow should end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4355138d-76c4-4995-ad99-1d93ed8a9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12149fc50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_finish_point(\"generate_cover_letter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74e5c8a-152b-4c6f-90e1-c06f41224cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKFLOW INFORMATION\n",
      "====================\n",
      "Nodes: {'generate_resume_summary': StateNodeSpec(runnable=generate_resume_summary(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None, input_schema=<class '__main__.ChainState'>, retry_policy=None, cache_policy=None, ends=(), defer=False), 'generate_cover_letter': StateNodeSpec(runnable=generate_cover_letter(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None, input_schema=<class '__main__.ChainState'>, retry_policy=None, cache_policy=None, ends=(), defer=False)}\n",
      "Edges: {('generate_cover_letter', '__end__'), ('generate_resume_summary', 'generate_cover_letter'), ('__start__', 'generate_resume_summary')}\n",
      "Finish points attribute not directly accessible\n"
     ]
    }
   ],
   "source": [
    "print_workflow_info(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12fcdf2-5f9d-4689-92b4-7d819cdef883",
   "metadata": {},
   "source": [
    "Now we are compiling the workflow into an executable app, which prepares it for running with input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dfda32b-69c6-41ce-8bcf-cbf0d6599772",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f36685c-11c5-4e12-8ede-db6ede80cff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install pygraphviz to draw graphs: `pip install pygraphviz`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py:138\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpygraphviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Generate a visualization of the workflow graph\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m display(Image(\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:574\u001b[0m, in \u001b[0;36mGraph.draw_png\u001b[0;34m(self, output_file_path, fontname, labels)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_png\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PngDrawer\n\u001b[1;32m    563\u001b[0m default_node_labels \u001b[38;5;241m=\u001b[39m {node\u001b[38;5;241m.\u001b[39mid: node\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPngDrawer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfontname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLabelsDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_node_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medges\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py:141\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    140\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall pygraphviz to draw graphs: `pip install pygraphviz`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[1;32m    144\u001b[0m viz \u001b[38;5;241m=\u001b[39m pgv\u001b[38;5;241m.\u001b[39mAGraph(directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nodesep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, ranksep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Install pygraphviz to draw graphs: `pip install pygraphviz`."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Generate a visualization of the workflow graph\n",
    "display(Image(app.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213c884-fbdd-4bc1-a2c1-22ded7ccf73c",
   "metadata": {},
   "source": [
    "Now we are defining the input for the workflow by providing a job description, and then invoking the compiled app with this input to run the entire prompt chaining process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a57cad3-94d9-4705-9e6b-592a77d8600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state = {\n",
    "        \"job_description\": \"We are looking for a data scientist with experience in machine learning, NLP, and Python. Prior work with large datasets and experience deploying models into production is required.\"\n",
    "}\n",
    "\n",
    "result = app.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224ac520-c465-4ee6-8746-3b22cbfb3824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s see. The user wants me to act as a resume assistant and summarize the key qualifications and experience from the job description into a resume summary from the applicant\\'s perspective.\\n\\nFirst, I need to parse the job description. The key points are: data scientist with experience in machine learning, NLP, Python. They require prior work with large datasets and deploying models into production.\\n\\nSo, the ideal candidate should highlight their expertise in those areas. The resume summary should be concise and use active language. Let me start by listing the main skills: machine learning, NLP, Python. Then, mention experience with large datasets and model deployment.\\n\\nI should also consider the structure. Typically, a resume summary starts with a role and years of experience, followed by key skills and achievements. Since the job requires deployment, maybe include a project or production experience example.\\n\\nWait, the user wants it phrased as if from the applicant\\'s perspective. So, using \"Experienced data scientist with...\" or \"Results-driven data scientist...\" as a starting point. Need to make sure to use action verbs and quantify achievements if possible, but the job description doesn\\'t provide specific metrics, so maybe keep it general.\\n\\nAlso, the job mentions deploying models into production. That\\'s important, so the summary should highlight that. Maybe something like \"proven ability to deploy machine learning models into production environments.\"\\n\\nLet me check if there are any other keywords. The job says \"prior work with large datasets,\" so the applicant should mention handling large datasets. Maybe \"proficient in processing and analyzing large datasets using Python.\"\\n\\nPutting it all together: Start with the role, mention the key technologies (Python, NLP, machine learning), experience with large datasets, and deployment. Maybe add a line about the impact, like optimizing models or improving performance.\\n\\nAvoid using the same phrasing as the job description. Instead of \"experience deploying models,\" use \"proven ability to deploy...\" or \"successfully deployed machine learning models into production.\"\\n\\nAlso, make sure it\\'s a single paragraph, concise, maybe 3-4 sentences. Let me draft a sample:\\n\\n\"Results-driven Data Scientist with 3+ years of experience in machine learning and natural language processing, specializing in Python-based solutions for complex data challenges. Proven ability to design, train, and deploy scalable ML models into production environments, optimizing performance for large-scale datasets. Adept at extracting actionable insights from big data, with a strong track record of delivering data-driven strategies that enhance business outcomes.\"\\n\\nDoes that cover all the points? Machine learning, NLP, Python, large datasets, deployment. Yes. Maybe add a mention of specific libraries or frameworks if allowed, but the job description doesn\\'t specify, so better to keep it general. Also, the sample includes quantified experience (3+ years), which is good. The user might not have specific numbers, but it\\'s common in summaries to include that. If the user doesn\\'t have years, they can adjust. But since the job requires experience, including a time frame makes sense.\\n\\nI think that\\'s a solid summary. Let me check for any missing elements. The job says \"required\" for deployment and large datasets, so those are must-haves. The summary addresses both. Also, the applicant\\'s perspective uses active language, which is correct for a resume summary.\\n</think>\\n\\n**Resume Summary:**  \\nResults-driven Data Scientist with 3+ years of expertise in machine learning, natural language processing (NLP), and Python, specializing in developing and deploying scalable models for real-world applications. Proven track record of analyzing and optimizing large datasets to drive actionable insights, with hands-on experience in end-to-end model development and production deployment. Skilled in leveraging cutting-edge algorithms and NLP techniques to solve complex problems, consistently delivering high-performance solutions that align with business objectives.  \\n\\n**Key Qualifications Highlighted:**  \\n- Advanced proficiency in machine learning and NLP using Python.  \\n- Demonstrated ability to work with large datasets and extract meaningful patterns.  \\n- Strong experience deploying models into production environments.  \\n- Focus on practical, scalable solutions with measurable impact.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['resume_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e062ce-2126-49f5-9ccd-03e004b62304",
   "metadata": {},
   "source": [
    "### Workflow Pattern: Routing\n",
    "\n",
    "Routing is a pattern where an LLM (or small router agent) **classifies or interprets incoming input** and then **routes it to the appropriate sub-process** or agent. This design is helpful when you're dealing with **multiple types of tasks or user intents**, and you want specialized logic or handling for each case.\n",
    "\n",
    "It works like a switchboard — one intelligent node (a classifier or router) analyzes the input and directs it to the correct branch.\n",
    "\n",
    "#### Use Cases:\n",
    "- AI customer service bots (route billing, tech support, or general inquiries)\n",
    "- Multi-skill agents (for example, summarization, translation, and data extraction)\n",
    "- Adaptive education bots (route to math, science, or grammar modules)\n",
    "\n",
    "---\n",
    "\n",
    "#### Routing Techniques:\n",
    "1. **Hard-coded keyword-based routing** (primitive)\n",
    "2. **LLM-based routing** using classification prompts\n",
    "3. **Embedding-based semantic matching** with a routing map\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58210f86-8542-4441-b42c-a8f9dc184387",
   "metadata": {},
   "source": [
    "### Use Case: Routing — Task Classifier for Summarization and Translation\n",
    "\n",
    "In this workflow, we are going to build a simple **task router** using the Routing design pattern. The goal is to create a system that can intelligently decide whether the user wants to **summarize** or **translate** a given input, and then send it to the appropriate processing path.\n",
    "\n",
    "This pattern is useful when the system needs to handle **multiple types of tasks** based on the user’s intent. Instead of creating one large model to handle everything, we let a **router node** classify the request and direct it to a **specialized sub-process**.\n",
    "\n",
    "For example:\n",
    "- If the input is “Summarize this article about AI,” the router sends it to the **summarizer**.\n",
    "- If the input is “Translate this to French,” it sends it to the **translator**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beaaee-7df0-4c19-ae05-933b2f580158",
   "metadata": {},
   "source": [
    "Now we are defining the `RouterState` using `TypedDict` to represent the shared state in our routing workflow. This state includes:\n",
    "\n",
    "- `user_input`: the raw input provided by the user,\n",
    "- `task_type`: the type of task determined by the router (for example, \"summarize\" or \"translate\"),\n",
    "- `output`: the final result generated after routing to the appropriate task handler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "048d8051-e135-4d48-8a1f-d847ec999e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterState(TypedDict):\n",
    "    user_input: str\n",
    "    task_type: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e62f0-2c8e-4416-8322-b1ac5d2aa81b",
   "metadata": {},
   "source": [
    "Now we are defining the `router_node`, which acts as the decision-maker in the workflow. It sends a prompt to the LLM asking it to classify the user's intent as either `\"summarize\"` or `\"translate\"`. The result is stored in the `task_type` field of the state and will determine which processing node the workflow routes to next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186190b-b272-414f-be56-0f915fffdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(BaseModel):\n",
    "    role: str = Field(..., description=\"Decide whether the user wants to summarize a passage  ouput 'summarize'  or translate text into French oupput translate.\")\n",
    "llm_router=llm.bind_tools([Router])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bb6a9a8-0eb9-4eda-b06b-adcefdb392f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'User wants translation. The system says we have a function Router that decides whether user wants to summarize or translate. We need to call it with role? The function expects role: string. Probably we need to pass role \"translate\". Then we will output translation. So call function.', 'tool_calls': [{'id': 'fc_65944293-2672-4f75-a3c6-d594887fb624', 'function': {'arguments': '{\"role\":\"translate\"}', 'name': 'Router'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 151, 'total_tokens': 234, 'completion_time': 0.166907088, 'prompt_time': 0.010392075, 'queue_time': 0.193940243, 'total_time': 0.177299163}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c367a3f8-ff68-41e5-8b04-066dce379946-0', tool_calls=[{'name': 'Router', 'args': {'role': 'translate'}, 'id': 'fc_65944293-2672-4f75-a3c6-d594887fb624', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 83, 'total_tokens': 234})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=llm_router.invoke(\"Please translate this: I love the sun its so warm\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f36c4154-e2c7-4c17-965f-5db84313cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: RouterState) -> RouterState:\n",
    "    routing_prompt = f\"\"\"\n",
    "    You are an AI task classifier.\n",
    "    \n",
    "    Decide whether the user wants to:\n",
    "    - \"summarize\" a passage\n",
    "    - or \"translate\" text into French\n",
    "    \n",
    "    Respond with just one word: 'summarize' or 'translate'.\n",
    "    \n",
    "    User Input: \"{state['user_input']}\"\n",
    "    \"\"\"\n",
    "    llm_router=llm.bind_tools([Router])\n",
    "    response = llm_router.invoke(routing_prompt)\n",
    "    print(response)\n",
    "    try:\n",
    "        state['task_type']=response.tool_calls[0]['args']['role']\n",
    "    except:\n",
    "        state['task_type']=response.content  # default to summarize if unsure\n",
    "\n",
    "    return {**state, \"task_type\": state['task_type']} # This becomes the next node's name!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758301eb-45e1-4077-852b-912a15b6a6a3",
   "metadata": {},
   "source": [
    "Now we are defining the `router` function, which simply returns the `task_type` from the state. This value will be used by LangGraph to decide which node to route to next based on the classification result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d095c645-bc32-45c9-bd9c-a3082b4d059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: RouterState) -> str:\n",
    "    return state['task_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10b4ec-6704-4082-8116-54d33aac723e",
   "metadata": {},
   "source": [
    "Now we are defining the `summarize_node`, which is responsible for summarizing the user's input. It prompts the LLM to generate a concise summary and stores the result in the `output` field, while also confirming the `task_type` as `\"summarize\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "472b5752-a7d0-4482-a1d6-c707ef824f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_node(state: RouterState) -> RouterState:\n",
    "    prompt = f\"Please summarize the following passage:\\n\\n{state['user_input']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state[\"task_type\"]=\"summarize\"\n",
    "    state[\"output\"]=response.content\n",
    "    \n",
    "    return {**state, \"task_type\": \"summarize\", \"output\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7e057-e957-4a2f-8c43-3ce4b84cb810",
   "metadata": {},
   "source": [
    "Now we are defining the `translate_node`, which handles translation tasks. It prompts the LLM to translate the user's input into French and saves the translated text in the `output` field, while also updating the `task_type` to `\"translate\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b9fe748-9ff9-4d5b-82c2-5f321e7365cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_node(state: RouterState) -> RouterState:\n",
    "    prompt = f\"Translate the following text to French:\\n\\n{state['user_input']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state[\"task_type\"]=\"translate\"\n",
    "    state[\"output\"]=response.content\n",
    "\n",
    "    return {**state, \"task_type\": \"translate\", \"output\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27354e76-f9cf-4084-b29d-e30fcd5dc15a",
   "metadata": {},
   "source": [
    "Now we are initializing a new `StateGraph` using the `RouterState` type. This sets up the structure of our routing workflow and defines the schema for the state that will be passed between nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6bbe865c-8400-42d6-829e-449bc93edf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(RouterState)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49765272-2ac5-49f4-81ff-2bc0ae7cfdca",
   "metadata": {},
   "source": [
    "Now we are adding three nodes to the workflow:\n",
    "\n",
    "- `\"router\"`: the node that classifies the user input,  \n",
    "- `\"summarize\"`: the node that handles summarization,  \n",
    "- `\"translate\"`: the node that handles translation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "46718db8-5871-44eb-9363-772de93d5493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1228702f0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"summarize\", summarize_node)\n",
    "workflow.add_node(\"translate\", translate_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d3459-cd29-4aa0-9e5c-94665d805bf4",
   "metadata": {},
   "source": [
    "Now we are setting `\"router\"` as the entry point of the workflow, meaning the execution will start by classifying the user’s intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "60e430c1-390a-40ce-a2cb-48ce552aa641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1228702f0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"router\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e76374-7bd9-461a-bab6-ab4f8f2e7c42",
   "metadata": {},
   "source": [
    "Now we are adding conditional edges from the `\"router\"` node based on the value returned by the `router` function. If the task is `\"summarize\"`, the workflow routes to the `summarize` node; if it's `\"translate\"`, it routes to the `translate` node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbff9eb0-d30c-4f4b-96c4-05eed923c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1228702f0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\"router\", router, {\n",
    "    \"summarize\": \"summarize\",\n",
    "    \"translate\": \"translate\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580b5ab-4af6-413f-9f1e-1bac0be79fbe",
   "metadata": {},
   "source": [
    "Now we are marking both `\"summarize\"` and `\"translate\"` as valid finish points. This means the workflow can end after executing either node, depending on the task type selected by the router.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2476340e-364e-4748-9f86-64092c508937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1228702f0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_finish_point(\"summarize\")\n",
    "workflow.set_finish_point(\"translate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba1edf-faa0-4bec-a2b1-47de5954b10f",
   "metadata": {},
   "source": [
    "We compile the workflow into an executable app using the `compile()` method. This step prepares the defined nodes and routing logic so the workflow can be run with actual input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2d616a55-273c-4357-a4cc-ef2c183e5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4b0a7-44bf-4108-a5aa-5c7088740663",
   "metadata": {},
   "source": [
    "To better understand the structure of our workflow, we use `app.get_graph().draw_png()` to generate a visual representation of the graph. This diagram helps us verify how nodes are connected and where conditional routing decisions lead within the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c9f004f-47fd-4e43-953e-fc26c8494c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install pygraphviz to draw graphs: `pip install pygraphviz`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py:138\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpygraphviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Generate a visualization of the workflow graph\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m display(Image(\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:574\u001b[0m, in \u001b[0;36mGraph.draw_png\u001b[0;34m(self, output_file_path, fontname, labels)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_png\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PngDrawer\n\u001b[1;32m    563\u001b[0m default_node_labels \u001b[38;5;241m=\u001b[39m {node\u001b[38;5;241m.\u001b[39mid: node\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPngDrawer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfontname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLabelsDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_node_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medges\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/IBM_course/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py:141\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    140\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall pygraphviz to draw graphs: `pip install pygraphviz`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[1;32m    144\u001b[0m viz \u001b[38;5;241m=\u001b[39m pgv\u001b[38;5;241m.\u001b[39mAGraph(directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nodesep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, ranksep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Install pygraphviz to draw graphs: `pip install pygraphviz`."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Generate a visualization of the workflow graph\n",
    "display(Image(app.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dfbfe3-df5a-4251-b8b2-3d8eb2ffbb27",
   "metadata": {},
   "source": [
    "In this step, we provide a minimal input containing only the `user_input` field. Since the state is defined using `TypedDict`, missing fields such as `task_type` and `output` will be automatically handled during execution. We then invoke the app with this input, which starts the routing process and executes the appropriate task based on the user's intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aab6494a-c61e-447a-80b1-9c13b0b0ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='translate' additional_kwargs={'reasoning_content': 'We need to decide if user wants summarize or translate. The user says \"Can you translate this sentence: I love programming?\" So it\\'s translate. Respond with just one word: \\'translate\\'.'} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 211, 'total_tokens': 260, 'completion_time': 0.103304671, 'prompt_time': 0.034182383, 'queue_time': 0.668233778, 'total_time': 0.137487054}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--a1f027c0-e9dc-4144-84fb-1360a59d6538-0' usage_metadata={'input_tokens': 211, 'output_tokens': 49, 'total_tokens': 260}\n"
     ]
    }
   ],
   "source": [
    "input_text = {\n",
    "        \"user_input\": \"Can you translate this sentence: I love programming?\"\n",
    "    }\n",
    "\n",
    "result = app.invoke(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a8b30928-5e9d-47c5-a844-f72885dd8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**French translation:**  \n",
      "*I love programming* → **« J’aime programmer »** (or, more emphatically, **« J’adore la programmation »**).\n",
      "translate\n"
     ]
    }
   ],
   "source": [
    "print(result[ 'output'])\n",
    "print(result['task_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9c8b6-1330-44a1-b0f1-5e7b5b5ee072",
   "metadata": {},
   "source": [
    "Let's try the second task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e868d2d0-075a-4ff7-8b39-e709d21c8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='summarize' additional_kwargs={'reasoning_content': 'We need to decide if user wants summarize or translate. The user says \"Can you summarize this sentence: ...\" So answer \"summarize\".'} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 227, 'total_tokens': 269, 'completion_time': 0.089942817, 'prompt_time': 0.159109944, 'queue_time': 0.56608529, 'total_time': 0.249052761}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--8eefd7c7-fe9b-4a2b-acd8-2c2673308cef-0' usage_metadata={'input_tokens': 227, 'output_tokens': 42, 'total_tokens': 269}\n"
     ]
    }
   ],
   "source": [
    "input_text = {\n",
    "        \"user_input\": \"Can you summarize this sentence: I love programming so much it is the best thing ever. All I want to do is programming?\"\n",
    "    }\n",
    "\n",
    "result = app.invoke(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8faf5405-5d51-4d8d-853d-d7c38abe0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker is extremely passionate about programming and wants to spend all their time doing it.\n",
      "summarize\n"
     ]
    }
   ],
   "source": [
    "print(result[ 'output'])\n",
    "print(result['task_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d7139-6f0d-4577-8d3c-b53531508f7b",
   "metadata": {},
   "source": [
    "### Workflow Pattern: Parallelization\n",
    "\n",
    "Parallelization is a pattern where **multiple LLM tasks are executed at the same time** instead of one after another. This is useful when different parts of the task can be done **independently**, allowing for faster processing and better system throughput.\n",
    "\n",
    "Think of it like a kitchen where one chef is chopping vegetables, another is boiling pasta, and another is baking — all at once. None of them need to wait for the other to finish.\n",
    "\n",
    "In AI workflows, this means breaking a problem into parts and running them **in parallel**, then collecting and combining the outputs.\n",
    "\n",
    "#### Use Cases:\n",
    "- Summarizing different sections of a large document simultaneously\n",
    "- Translating a batch of user messages at once\n",
    "- Generating multiple variations of an ad copy or product description\n",
    "- Running safety checks using different prompts and comparing outputs\n",
    "- Ensembling results from different models or prompts for consensus\n",
    "\n",
    "---\n",
    "\n",
    "#### Parallelization Techniques:\n",
    "1. **Format Diversity (Multi-Output Tasks)**  \n",
    "   - Run the **same input** through **different prompt styles, languages, or output formats**  \n",
    "   - Each LLM call produces a distinct kind of result in parallel  \n",
    "   - Combine all outputs into a unified response  \n",
    "   - *Example:* Translate a sentence into French, Spanish, and Japanese at the same time\n",
    "\n",
    "2. **Task Splitting (Sectioning)**  \n",
    "   - Divide a large input into smaller parts  \n",
    "   - Run each part through the same task (for example, summarization) in parallel  \n",
    "   - Merge the partial results for a final outcome  \n",
    "   - *Example:* Summarize each paragraph of an article simultaneously\n",
    "\n",
    "3. **Consensus Voting (Multi-Agent Evaluation)**  \n",
    "   - Run the **same task** multiple times with different agents or prompt styles  \n",
    "   - Compare the responses and choose the best one via ranking or majority vote  \n",
    "   - *Example:* Ask 3 variations of a model to write a safe response and pick the most appropriate one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95effb21-14ca-4300-a6f3-d4888ff2d3c7",
   "metadata": {},
   "source": [
    "### ⚡ Use Case: Parallelization — Multilingual Translation Assistant\n",
    "\n",
    "In this workflow, we are building a **multilingual translation assistant** using the Parallelization design pattern. The goal is to take a single English sentence and generate its translations into **French**, **Spanish**, and **Japanese** — all at the same time.\n",
    "\n",
    "Parallelization is ideal for this task because the translations are **independent of one another**. Since each language-specific translation can be processed separately, we can run them in **parallel** to save time and improve efficiency.\n",
    "\n",
    "Once all translations are completed, we aggregate the outputs into a **single multilingual result**, making it easy to present or store all versions together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ac125-ef45-4eff-adae-a09a7bbdc554",
   "metadata": {},
   "source": [
    "To manage the data flowing through our parallel translation workflow, we define a `State` using `TypedDict`. This structure allows us to keep track of both the input and the individual outputs from each parallel task.\n",
    "\n",
    "- `text`: stores the original English sentence to be translated\n",
    "- `french`: will hold the French translation\n",
    "- `spanish`: will hold the Spanish translation\n",
    "- `japanese`: will hold the Japanese translation\n",
    "- `combined_output`: will store the final combined result of all translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "62c2f441-15a9-415f-addb-5d07704dc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    text: str\n",
    "    french: str\n",
    "    spanish: str\n",
    "    japanese: str\n",
    "    combined_output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ff1ab-4fa8-42b1-8f4d-2f6fef30cd20",
   "metadata": {},
   "source": [
    "Here, we define the `translate_french` node, which is responsible for translating the input text into French. It sends a prompt to the LLM asking for the French version of the input, and then stores the result in the `french` field of the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f90535c4-366f-4d56-a530-de8a7cb5f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_french(state: State) -> dict:\n",
    "    response = llm.invoke(f\"Translate the following text to French:\\n\\n{state['text']}\")\n",
    "    return {\"french\": response.content.strip()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f597c5-163d-43b0-84d5-1a1463db3b57",
   "metadata": {},
   "source": [
    "In the same way, we define the `translate_spanish` node to handle translation into Spanish. It prompts the LLM with the input text and stores the translated result in the `spanish` field of the state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3dfb374c-500f-402c-bea4-14673d71989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_spanish(state: State) -> dict:\n",
    "    response = llm.invoke(f\"Translate the following text to Spanish:\\n\\n{state['text']}\")\n",
    "    return {\"spanish\": response.content.strip()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421803b5-63d9-4eb9-a752-5661288ac68e",
   "metadata": {},
   "source": [
    "Similarly, we’ll create a `translate_japanese` node to translate the input text into Japanese and store the result in the `japanese` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "824bfa45-54fc-4e9a-852d-34be12561178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_japanese(state: State) -> dict:\n",
    "    response = llm.invoke(f\"Translate the following text to Japanese:\\n\\n{state['text']}\")\n",
    "    return {\"japanese\": response.content.strip()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6d74f-117a-4d27-8466-4d3c9156a92b",
   "metadata": {},
   "source": [
    "The `aggregator` node is responsible for combining the outputs from all three translation nodes into a single, readable format. It constructs a formatted string that includes the original text along with its French, Spanish, and Japanese translations, and stores the final result in the `combined_output` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ccd17ec-45d3-4060-b8cc-ee9b7cd209a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(state: State) -> dict:\n",
    "    combined = f\"Original Text: {state['text']}\\n\\n\"\n",
    "    combined += f\"French: {state['french']}\\n\\n\"\n",
    "    combined += f\"Spanish: {state['spanish']}\\n\\n\"\n",
    "    combined += f\"Japanese: {state['japanese']}\\n\"\n",
    "    return {\"combined_output\": combined}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad679fc-acfe-4c2d-8481-60fa54aa69ba",
   "metadata": {},
   "source": [
    "We initialize a new `StateGraph` using the `State` type. This sets up the foundation for our parallel workflow and defines the structure of the data that will flow between nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aa2ae597-a5c3-4edc-a37e-da89e1f4e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110e7c7-05e5-4856-96ed-0e2332c5123d",
   "metadata": {},
   "source": [
    "We add four nodes to the graph:\n",
    "\n",
    "- `\"translate_french\"`: handles translation to French,  \n",
    "- `\"translate_spanish\"`: handles translation to Spanish,  \n",
    "- `\"translate_japanese\"`: handles translation to Japanese,  \n",
    "- `\"aggregator\"`: collects and combines all translations into one output.\n",
    "\n",
    "Each of these nodes will operate independently before merging at the aggregator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6aa4be55-3a88-434e-b288-d6b5bc20058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12282b8c0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node(\"translate_french\", translate_french)\n",
    "graph.add_node(\"translate_spanish\", translate_spanish)\n",
    "graph.add_node(\"translate_japanese\", translate_japanese)\n",
    "graph.add_node(\"aggregator\", aggregator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405396ad-283b-435b-8d36-cc74f7112917",
   "metadata": {},
   "source": [
    "We connect the three translation nodes to the `START` of the workflow. This means all translation tasks will begin simultaneously in parallel as soon as the workflow is triggered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cdf6d5c4-f4af-40c0-86ba-2755671aa3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12282b8c0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect parallel nodes from START\n",
    "graph.add_edge(START, \"translate_french\")\n",
    "graph.add_edge(START, \"translate_spanish\")\n",
    "graph.add_edge(START, \"translate_japanese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ab3b5-0aca-451f-83fe-e066f78a3d03",
   "metadata": {},
   "source": [
    "Here, we connect each of the translation nodes to the `aggregator` node. Once all translations are complete, their outputs will be passed to the aggregator to be combined into the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "337bca40-d317-40bf-88ee-54ca0c5dea15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12282b8c0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect all translation nodes to the aggregator\n",
    "graph.add_edge(\"translate_french\", \"aggregator\")\n",
    "graph.add_edge(\"translate_spanish\", \"aggregator\")\n",
    "graph.add_edge(\"translate_japanese\", \"aggregator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7756c4-1a60-4afc-9d99-45d86144b3cc",
   "metadata": {},
   "source": [
    "We connect the `aggregator` node to the `END` of the workflow, marking it as the final step where the combined output is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ac6d901-b83f-41f0-bf70-fde39f6317d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12282b8c0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final node\n",
    "graph.add_edge(\"aggregator\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ad3e3449-51a0-4edb-ace0-0809ea449a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637ebd8-d215-4434-93c3-b415d0b166ba",
   "metadata": {},
   "source": [
    "We provide an English sentence as input under the `text` field and invoke the workflow. This triggers all translation tasks in parallel, and once completed, the translations are combined and returned as the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "12222781-9ce8-4434-86b1-d20e62e5e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = {\n",
    "        \"text\": \"Good morning! I hope you have a wonderful day.\"\n",
    "}\n",
    "\n",
    "result = app.invoke(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8a0e2be4-4a64-4ce8-bf39-7c77cc239f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Good morning! I hope you have a wonderful day.',\n",
       " 'french': \"Bonjour\\u202f! J'espère que vous passerez une journée merveilleuse.\",\n",
       " 'spanish': '¡Buenos días! Espero que tengas un día maravilloso.',\n",
       " 'japanese': 'おはようございます！素敵な一日になりますように。',\n",
       " 'combined_output': \"Original Text: Good morning! I hope you have a wonderful day.\\n\\nFrench: Bonjour\\u202f! J'espère que vous passerez une journée merveilleuse.\\n\\nSpanish: ¡Buenos días! Espero que tengas un día maravilloso.\\n\\nJapanese: おはようございます！素敵な一日になりますように。\\n\"}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4defbfcc-5e24-4a5c-a756-3ded488f56bf",
   "metadata": {},
   "source": [
    "### Exercises: Building a Multi-Agent Routing System\n",
    "\n",
    "\n",
    "#### Exercise 1 - Create State Management and Router Tool\n",
    "Define the state structure and classification tool for your multi-agent routing system. You need to create a TypedDict for state management and a Pydantic model for LLM tool binding.\n",
    "\n",
    "Your task: Create the foundational components for routing between ride hailing, restaurant orders, groceries, and default handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c1e66-b949-4bd6-8a40-9beba5dcbf09",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    user_input: str\n",
    "    task_type: str\n",
    "    output: str\n",
    "\n",
    "class Router(BaseModel):\n",
    "    role: str = Field(\n",
    "        ..., \n",
    "        description=\"Classify the user request. Return exactly one of: 'ride_hailing_call', 'restaurant_order', 'groceries' and if you do not know output 'default_handler'\"\n",
    "    )\n",
    "\n",
    "llm_router = llm.bind_tools([Router])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e3b4f-110d-4924-b4b6-acfd1d65db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your RouterState TypedDict with three fields\n",
    "# TODO: Create a Router BaseModel with proper field description\n",
    "# TODO: Bind the router tool to your LLM\n",
    "class RouterState(TypedDict):\n",
    "    user_input: str\n",
    "    task_type: str\n",
    "    output: str\n",
    "\n",
    "class Router(BaseModel):\n",
    "    role: str = Field(\n",
    "        ..., \n",
    "        description=\"Classify the user request. Return exactly one of: 'ride_hailing_call', 'restaurant_order', 'groceries' and if you do not know output 'default_handler'\"\n",
    "    )\n",
    "\n",
    "llm_router = llm.bind_tools([Router])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3623ef2-91b7-47bd-bce7-c71c39e0176d",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2 - Implement Router Logic\n",
    "\n",
    "Create the router node function that classifies user input and handles cases where the LLM doesn't return a tool call. Also implement the router decision function.\n",
    "\n",
    "Your task: Build the classification logic with proper error handling for unclassified requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc99597-27ed-4a90-9e7f-e594f5d700ab",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def router_node(state: RouterState) -> RouterState:\n",
    "    response = llm_router.invoke(state['user_input'])\n",
    "    \n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]['args']['role']\n",
    "        return {**state, \"task_type\": tool_call}\n",
    "    else:\n",
    "        return {**state, \"task_type\": \"default_handler\"}\n",
    "\n",
    "def router(state: RouterState) -> str:\n",
    "    return state['task_type']\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce036f-3625-4f20-9425-de216a8f820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement router_node function with tool call handling\n",
    "# TODO: Add fallback to \"default_handler\" when no tool calls\n",
    "# TODO: Create router function that returns task_type from state\n",
    "def router_node(state: RouterState) -> RouterState:\n",
    "    response = llm_router.invoke(state['user_input'])\n",
    "    \n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]['args']['role']\n",
    "        return {**state, \"task_type\": tool_call}\n",
    "    else:\n",
    "        return {**state, \"task_type\": \"default_handler\"}\n",
    "\n",
    "def router(state: RouterState) -> str:\n",
    "    return state['task_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b983d1b-5b78-4e7d-a112-4b6c79681192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ride_hailing_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes ride hailing requests by extracting pickup/dropoff locations and preferences\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a ride hailing assistant. Based on the user's request, extract and organize the following information:\n",
    "    \n",
    "    - Pickup location\n",
    "    - Destination/dropoff location  \n",
    "    - Preferred ride type (if mentioned)\n",
    "    - Any special requirements\n",
    "    - Estimated timing preferences\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a clear summary of the ride request with all available details.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"ride_hailing_call\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "\n",
    "def restaurant_order_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes restaurant orders by organizing menu items, quantities, and preferences\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a restaurant ordering assistant. Based on the user's request, organize the following information:\n",
    "    \n",
    "    - Menu items requested\n",
    "    - Quantities for each item\n",
    "    - Special modifications or dietary restrictions\n",
    "    - Delivery or pickup preference\n",
    "    - Any timing requirements\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a clear, organized summary of the restaurant order with all details.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"restaurant_order\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "\n",
    "def groceries_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes grocery delivery requests with driver pickup service\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a grocery delivery assistant for a service where our drivers pick up groceries for customers.\n",
    "    \n",
    "    Based on the user's request, organize the following information:\n",
    "    \n",
    "    Shopping List:\n",
    "    - List of grocery items needed\n",
    "    - Quantities or amounts for each item\n",
    "    - Brand preferences (if mentioned)\n",
    "    - Any dietary restrictions or organic preferences\n",
    "    \n",
    "    Store Information:\n",
    "    - Preferred store or location\n",
    "    - Budget considerations\n",
    "    - Special instructions for finding items\n",
    "    \n",
    "    Delivery Details:\n",
    "    - Delivery address (if provided)\n",
    "    - Preferred delivery time window\n",
    "    - Any special delivery instructions\n",
    "    - Contact information for driver coordination\n",
    "    \n",
    "    Driver Instructions:\n",
    "    - Substitution preferences (if item unavailable)\n",
    "    - How to handle out-of-stock items\n",
    "    - Any items requiring special handling (fragile, cold items)\n",
    "    - Payment method (if mentioned)\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a comprehensive delivery order summary that our driver can use to efficiently shop and deliver groceries. \n",
    "    Include estimated pickup time and any special notes for the shopping trip.\n",
    "    \n",
    "    Format the response as a clear, organized delivery order that includes all necessary details for our driver service.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"groceries\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "def default_handler_node(state: RouterState) -> RouterState:\n",
    "    prompt = f\"\"\"\n",
    "    I couldn't classify your request into a specific category. \n",
    "    Let me provide general assistance for: \"{state['user_input']}\"\n",
    "    \n",
    "    I can help you with:\n",
    "    - Ride hailing services\n",
    "    -  Restaurant orders  \n",
    "    -  Grocery shopping\n",
    "    \n",
    "    Please rephrase your request to match one of these services, or if you need assistance with something else, I will connect you with our customer support team who can provide personalized help.\n",
    "    \n",
    "    Would you like me to:\n",
    "    1. Help you rephrase your request for one of our services\n",
    "    2. Connect you with customer support for additional assistance\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"task_type\": \"default_handler\", \"output\": response.content.strip()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7cf96a-70ba-42c9-8ca8-01e3666393cd",
   "metadata": {},
   "source": [
    "#### Exercise 3 - Assemble the Complete Workflow\n",
    "\n",
    "Put all the pieces together by building the StateGraph, adding nodes, setting up routing logic, and compiling the application.\n",
    "\n",
    "Your task: Create the complete workflow graph with proper routing and finish points, and call it ```app```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d2954-b966-4f2a-bcc1-10ffa391dc6c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "workflow = StateGraph(RouterState)\n",
    "\n",
    "# Add all nodes\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"ride_hailing_call\", ride_hailing_node)\n",
    "workflow.add_node(\"restaurant_order\", restaurant_order_node)\n",
    "workflow.add_node(\"groceries\", groceries_node)\n",
    "workflow.add_node(\"default_handler\", default_handler_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Add conditional routing\n",
    "workflow.add_conditional_edges(\"router\", router, {\n",
    "    \"groceries\": \"groceries\", \n",
    "    \"restaurant_order\": \"restaurant_order\",\n",
    "    \"ride_hailing_call\": \"ride_hailing_call\",\n",
    "    \"default_handler\": \"default_handler\"\n",
    "})\n",
    "\n",
    "# Set finish points\n",
    "workflow.set_finish_point(\"ride_hailing_call\")\n",
    "workflow.set_finish_point(\"restaurant_order\")\n",
    "workflow.set_finish_point(\"groceries\")\n",
    "workflow.set_finish_point(\"default_handler\")\n",
    "\n",
    "# Compile the application\n",
    "app = workflow.compile()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef7a0c-38df-4901-9958-6bfc7448471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create StateGraph with RouterState\n",
    "# TODO: Add all five nodes (router + 4 processing nodes)\n",
    "# TODO: Set router as entry point\n",
    "# TODO: Add conditional edges with all four routing options\n",
    "# TODO: Set finish points for all processing nodes\n",
    "# TODO: Compile the application\n",
    "workflow = StateGraph(RouterState)\n",
    "# Add all nodes\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"ride_hailing_call\", ride_hailing_node)\n",
    "workflow.add_node(\"restaurant_order\", restaurant_order_node)\n",
    "workflow.add_node(\"groceries\", groceries_node)\n",
    "workflow.add_node(\"default_handler\", default_handler_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Add conditional routing\n",
    "workflow.add_conditional_edges(\"router\", router, {\n",
    "    \"groceries\": \"groceries\", \n",
    "    \"restaurant_order\": \"restaurant_order\",\n",
    "    \"ride_hailing_call\": \"ride_hailing_call\",\n",
    "    \"default_handler\": \"default_handler\"\n",
    "})\n",
    "\n",
    "# Set finish points\n",
    "workflow.set_finish_point(\"ride_hailing_call\")\n",
    "workflow.set_finish_point(\"restaurant_order\")\n",
    "workflow.set_finish_point(\"groceries\")\n",
    "workflow.set_finish_point(\"default_handler\")\n",
    "\n",
    "# Compile the application\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2c19b-6cfc-49e6-8053-9127dfd9e288",
   "metadata": {},
   "source": [
    "Test your implementation here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae419b8-f833-418f-84e1-f401ac66343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\"user_input\": \"I need a ride from downtown to the airport at 3pm\"},\n",
    "    {\"user_input\": \"I want to order 2 large pepperoni pizzas for delivery\"},\n",
    "    {\"user_input\": \"I need milk, bread, eggs, and vegetables for the week\"},\n",
    "    {\"user_input\": \"What's the weather like today?\"},  # Default/unclassified example\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    result=app.invoke(test_input)\n",
    "\n",
    "\n",
    "    print(f\"question {test_input[\"user_input\"]}\\n\")\n",
    "    print(f\"task_type {result['task_type']}\\n\")\n",
    "    print(f\"output: {result['output']}\\n\")\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872fc6-abf8-485a-9301-e8d9465f6887",
   "metadata": {},
   "source": [
    "Here is the complete code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76aead-e3de-496d-b558-c1cbc585ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterState(TypedDict):\n",
    "    user_input: str\n",
    "    task_type: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class Router(BaseModel):\n",
    "    role: str = Field(\n",
    "        ..., \n",
    "        description=\"Classify the user request. Return exactly one of: 'ride_hailing_call', 'restaurant_order',  'groceries' and if you do not know output  'default_handler'\"\n",
    "    )\n",
    "\n",
    "llm_router = llm.bind_tools([Router])\n",
    "\n",
    "def router_node(state: RouterState) -> RouterState:\n",
    "    response = llm_router.invoke(state['user_input'])\n",
    "    \n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]['args']['role']\n",
    "        return {**state, \"task_type\": tool_call}\n",
    "    else:\n",
    "        return {**state, \"task_type\": \"default_handler\"}\n",
    "\n",
    "\n",
    "def router(state: RouterState) -> str:\n",
    "    return state['task_type']\n",
    "    \n",
    "def ride_hailing_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes ride hailing requests by extracting pickup/dropoff locations and preferences\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a ride hailing assistant. Based on the user's request, extract and organize the following information:\n",
    "    \n",
    "    - Pickup location\n",
    "    - Destination/dropoff location  \n",
    "    - Preferred ride type (if mentioned)\n",
    "    - Any special requirements\n",
    "    - Estimated timing preferences\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a clear summary of the ride request with all available details.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"ride_hailing_call\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "\n",
    "def restaurant_order_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes restaurant orders by organizing menu items, quantities, and preferences\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a restaurant ordering assistant. Based on the user's request, organize the following information:\n",
    "    \n",
    "    - Menu items requested\n",
    "    - Quantities for each item\n",
    "    - Special modifications or dietary restrictions\n",
    "    - Delivery or pickup preference\n",
    "    - Any timing requirements\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a clear, organized summary of the restaurant order with all details.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"restaurant_order\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "\n",
    "def groceries_node(state: RouterState) -> RouterState:\n",
    "    \"\"\"\n",
    "    Processes grocery delivery requests with driver pickup service\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a grocery delivery assistant for a service where our drivers pick up groceries for customers.\n",
    "    \n",
    "    Based on the user's request, organize the following information:\n",
    "    \n",
    "    Shopping List:\n",
    "    - List of grocery items needed\n",
    "    - Quantities or amounts for each item\n",
    "    - Brand preferences (if mentioned)\n",
    "    - Any dietary restrictions or organic preferences\n",
    "    \n",
    "    Store Information:\n",
    "    - Preferred store or location\n",
    "    - Budget considerations\n",
    "    - Special instructions for finding items\n",
    "    \n",
    "    Delivery Details:\n",
    "    - Delivery address (if provided)\n",
    "    - Preferred delivery time window\n",
    "    - Any special delivery instructions\n",
    "    - Contact information for driver coordination\n",
    "    \n",
    "    Driver Instructions:\n",
    "    - Substitution preferences (if item unavailable)\n",
    "    - How to handle out-of-stock items\n",
    "    - Any items requiring special handling (fragile, cold items)\n",
    "    - Payment method (if mentioned)\n",
    "    \n",
    "    User Request: \"{state['user_input']}\"\n",
    "    \n",
    "    Provide a comprehensive delivery order summary that our driver can use to efficiently shop and deliver groceries. \n",
    "    Include estimated pickup time and any special notes for the shopping trip.\n",
    "    \n",
    "    Format the response as a clear, organized delivery order that includes all necessary details for our driver service.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"task_type\": \"groceries\", \n",
    "        \"output\": response.content.strip()\n",
    "    }\n",
    "def default_handler_node(state: RouterState) -> RouterState:\n",
    "    prompt = f\"\"\"\n",
    "    I couldn't classify your request into a specific category. \n",
    "    Let me provide general assistance for: \"{state['user_input']}\"\n",
    "    \n",
    "    I can help you with:\n",
    "    - Ride hailing services\n",
    "    -  Restaurant orders  \n",
    "    -  Grocery shopping\n",
    "    \n",
    "    Please rephrase your request to match one of these services, or if you need assistance with something else, I will connect you with our customer support team who can provide personalized help.\n",
    "    \n",
    "    Would you like me to:\n",
    "    1. Help you rephrase your request for one of our services\n",
    "    2. Connect you with customer support for additional assistance\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"task_type\": \"default_handler\", \"output\": response.content.strip()}\n",
    "\n",
    "workflow = StateGraph(RouterState)\n",
    "workflow.add_node(\"ride_hailing_call\", ride_hailing_node)\n",
    "workflow.add_node(\"restaurant_order\", restaurant_order_node)\n",
    "workflow.add_node(\"groceries\", groceries_node)\n",
    "workflow.add_node(\"default_handler\", default_handler_node)\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.set_entry_point(\"router\")\n",
    "# Update your conditional edges to route to all three options\n",
    "workflow.add_conditional_edges(\"router\", router, {\"groceries\":\"groceries\", \"restaurant_order\":\"restaurant_order\",\"ride_hailing_call\":\"ride_hailing_call\",\"default_handler\":\"default_handler\"})\n",
    "\n",
    "\n",
    "# Set all three as possible finish points\n",
    "\n",
    "workflow.set_finish_point(\"ride_hailing_call\")\n",
    "workflow.set_finish_point(\"restaurant_order\")\n",
    "workflow.set_finish_point(\"groceries\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "test_cases = [\n",
    "    {\"user_input\": \"I need a ride from downtown to the airport at 3pm\"},\n",
    "    {\"user_input\": \"I want to order 2 large pepperoni pizzas for delivery\"},\n",
    "    {\"user_input\": \"I need milk, bread, eggs, and vegetables for the week\"},\n",
    "    {\"user_input\": \"What's the weather like today?\"},  # Default/unclassified example\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    result=app.invoke(test_input)\n",
    "\n",
    "\n",
    "    print(f\"question {test_input[\"user_input\"]}\\n\")\n",
    "    print(f\"task_type {result['task_type']}\\n\")\n",
    "    print(f\"output: {result['output']}\\n\")\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ad248-0d77-48de-937c-81403dbd4294",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "<a href=\"https://author.skills.network/instructors/kunal_makwana\" target=\"_blank\">Kunal Makwana</a> is a passionate Data Scientist and AI enthusiast, currently working at IBM on innovative projects in Generative AI and machine learning. His journey began with a deep interest in mathematics and coding, which inspired him to explore how data can solve real-world problems. Over the years, he's gained hands-on experience in building scalable AI solutions, fine-tuning models, and leveraging cloud technologies to extract meaningful insights from complex datasets.\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\" target=\"_blank\">Joseph Santarcangelo</a> has a  Ph.D. in Electrical Engineering with his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a16c9a-5722-45d2-9275-403131dc55a0",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for the changelog</summary>\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-07-17|0.1|Kunal Makwana|Initial version created|\n",
    "|2025-07-22|0.2|Steve Ryan|ID review|\n",
    "|2025-07-22|0.3|Leah Hanson|QA review|\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092398b-5ee9-428f-aa11-874929950098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBM_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "prev_pub_hash": "af2f53da5f34d69c384ef6f644fe5a7bc7c06928f1e87aaad2cfd992a92045cf"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
